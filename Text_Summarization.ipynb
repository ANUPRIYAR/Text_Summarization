{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Summarization.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNFt2q6dZLSx8sFkU210yfO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ANUPRIYAR/Text_Summarization/blob/main/Text_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtL-n7H1RV_K"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import networkx as nx"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OJ-IRQufx-x"
      },
      "source": [
        "**Algorithm for Extractive Summarization:**\n",
        "\n",
        "Input Article -> split into sentences -> Remove Stop words -> Build a smilarity Matrix -> Generate Rank based on Matrix  -> Pick top N sentences for Summary \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCFsicUBxR8S"
      },
      "source": [
        "def read_article(file_name):\n",
        "  ifile = open(file_name, 'r')\n",
        "  lines = ifile.readlines()\n",
        "  # print(lines)\n",
        "\n",
        "  # Splitting article into a list of sentences\n",
        "  article = lines[0].split('.')\n",
        "  # print(article)\n",
        "  sentences = []\n",
        "\n",
        "  # splitting words in each sentence\n",
        "  for sentence in article:\n",
        "    sentences.append(sentence.split(\" \"))\n",
        "    sentences.pop\n",
        "  return sentences"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml_ZAgFP8RTG"
      },
      "source": [
        "def clean_text(sentences):\n",
        "\n",
        "  nltk.download('stopwords')\n",
        "  stop_words = stopwords.words('english')\n",
        "\n",
        "  #  Remove stopwords\n",
        "\n",
        "  for sentence in sentences:\n",
        "    for word in sentence:\n",
        "      if word in stop_words:\n",
        "        sentence.remove(word)\n",
        "\n",
        "  return sentences\n",
        "      "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsdjFjfmCLkL"
      },
      "source": [
        "# !wget https://raw.githubusercontent.com/edubey/text-summarizer/master/trump.txt"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bzpoQOoE6UP"
      },
      "source": [
        "def sentence_similarity(sent1, sent2):\n",
        "  # print(\"sent1:\\n\",  sent1)\n",
        "  # print(\"sent2:\\n\",sent2)\n",
        "  sent1 = [w.lower for w in sent1]\n",
        "  sent2 = [w.lower for w in sent2]\n",
        "\n",
        "  all_words= list(set(sent1 + sent2))\n",
        "\n",
        "  # Convert sent1 and sent2 into vectors\n",
        "  vector1 = [0]* len(all_words)\n",
        "  vector2 = [0] * len(all_words)\n",
        "  # print(\"vcetor1 when initialized: \", vector1)\n",
        "  for w in sent1:\n",
        "    vector1[all_words.index(w)] += 1\n",
        "  # print(\"vector1 final :\",vector1)\n",
        "\n",
        "  for w in sent2:\n",
        "    vector2[all_words.index(w)] += 1\n",
        "\n",
        "  return 1 - cosine_distance(vector1,vector2)\n",
        "\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9_iqXiiN42a"
      },
      "source": [
        "def build_similarity_matrix(sentences):\n",
        "  # Create an empty similarity matrix\n",
        "  similarity_matrix = np.zeros((len(sentences),len(sentences)))\n",
        "  \n",
        "  for idx1 in range(len(sentences)) : \n",
        "    for idx2 in range(len(sentences)):\n",
        "      if idx1 == idx2:\n",
        "        continue\n",
        "\n",
        "      similarity_matrix[idx1, idx2] = sentence_similarity(sentences[idx1],sentences[idx2])\n",
        "\n",
        "  return similarity_matrix\n",
        "\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUAPOkFAPnsN"
      },
      "source": [
        "def generate_summary(file_name, top_n=5):\n",
        "\n",
        "  #  Step 1 Read text and split it\n",
        "  sentences = read_article(file_name)\n",
        "\n",
        "  # Build similarity Matrix\n",
        "  sentence_similarity_matrix = build_similarity_matrix(sentences)\n",
        "\n",
        "  # Rank sentences in similarity Matrix\n",
        "  sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
        "  # print(\"sentence_similarity_graph :\", sentence_similarity_graph)\n",
        "  scores = nx.pagerank(sentence_similarity_graph)\n",
        "  # print(\"scores:\",scores)\n",
        "\n",
        "  # Step4 Sort the rank annd pick top 4 sentences\n",
        "  ranked_sentence = sorted(((scores[i], s) for i,s in enumerate(sentences)), reverse=True)\n",
        "  # print(\"Indexes of top ranked_sentence order are \", ranked_sentence)  \n",
        "\n",
        "\n",
        "  summarize_text = []\n",
        "  for i in range(top_n):\n",
        "    summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
        "  \n",
        "  # print(\"\\n \\n \\n ===================================\")\n",
        "\n",
        "\n",
        "  print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWnwUiiqffHN",
        "outputId": "f4890779-2ea2-43b1-cc8d-5525745e92cd"
      },
      "source": [
        "# !wget https://raw.githubusercontent.com/edubey/text-summarizer/master/fb.txt"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-02 04:37:34--  https://raw.githubusercontent.com/edubey/text-summarizer/master/fb.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1424 (1.4K) [text/plain]\n",
            "Saving to: ‘fb.txt.2’\n",
            "\n",
            "\rfb.txt.2              0%[                    ]       0  --.-KB/s               \rfb.txt.2            100%[===================>]   1.39K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-12-02 04:37:34 (23.1 MB/s) - ‘fb.txt.2’ saved [1424/1424]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-ZOsqtceM9_",
        "outputId": "58c7d5f4-7c5f-48e7-fab8-be73060b5ce6"
      },
      "source": [
        "generate_summary('fb.txt')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summarize Text: \n",
            " .  The exchange was intended to benefit everyone.  Partner companies acquired features to make their products more attractive.  Facebook users connected with friends across different devices and websites.  Pushing for explosive growth, Facebook got more users, lifting its advertising revenue\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ja38YRUgM6r"
      },
      "source": [
        ""
      ],
      "execution_count": 37,
      "outputs": []
    }
  ]
}